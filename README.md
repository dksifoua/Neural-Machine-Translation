# The State of the Art in Neural Machine Translation

## Abstract
Neural Machine Translation (NMT) has rapidly evolved to become the predominant approach in automated translation, offering a powerful end-to-end learning paradigm that has largely surpassed traditional phrase-based systems. This paper outlines the significant advancements in NMT, tracing its progression from initial encoder-decoder architectures with recurrent neural networks (RNNs) to sophisticated models leveraging attention mechanisms, convolutional neural networks, and self-attention. Key milestones include the introduction of attention to overcome fixed-length vector bottlenecks, the development of deep LSTM networks with techniques for production-scale deployment like Google's NMT (GNMT), the exploration of fully convolutional architectures (ConvS2S) for improved parallelization, and the transformative "Transformer" model, which relies entirely on attention. Recent work further enhances performance by identifying and combining effective modeling and training techniques from these diverse architectures into hybrid models. Despite achieving human-level quality in some contexts, challenges remain, particularly concerning robust rare word handling and deeper linguistic understanding.

# TODO
- [] Sequence to Sequence Learning with Neural Networks
- [] Neural Machine Translation by Jointly Learning to Align and Translate
- [] Effective Approaches to Attention-based Neural Machine Translation
- [] Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation
- [] Convolutional sequence to sequence learning.
- [] Attention is all you need.
- [] The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation

# References

- [1] Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Advances in neural information processing systems (pp. 3104-3112).
- [2] Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.
- [3] Luong, M. T., Pham, H., & Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025.
- [4] Gehring, J., Auli, M., Grangier, D., Yarats, D., & Dauphin, Y. N. (2017). Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122.
- [5] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008).


